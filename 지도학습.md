### 지도학습

---

**모델 복잡도와 데이터셋 크기의 관계**

모델의 복잡도는 훈련 데이터셋에 담긴 입력 데이터의 다양성과 관련이 깊음

데이터셋에 다양한 데이터 포인트가 많을수록(데이터가 많을 수록) 과대적합 없이 더 복잡한 모델을 만들 수 있음

다양성을 키워주므로 큰 데이터셋은 더 복잡한 모델을 만들 수 있게 해줌



### 선형모델

=> 입력 특성에 대한 선형 함수를 만들어 예측



### 회귀의 선형 모델

선형모델을 위한 일반화된 예측 함수

**y = w[0] \* x[0] + w[1] * x[1] ... w[p] * x[p] + b;**

x[0] ~ x[p] 까지는 데이터 포인트에 대한 **특성**

w와 b는 모델이 학습할 파라미터

그리고 y은 모델이 만들어낸 예측값

하나라면 

y = w[0] * x[0] + b

곧, 예측값은 입력 특성에 w의 각 가중치를 곱해서 더한 가중치 합으로 볼 수 있음



### 1.선형 회귀(최소제곱법)

예측과 훈련 세트에 있는 타깃 y 사이의 **평균제곱오차**(mean squared error)를 최소화하는 파라미터 w와 b를 찾는 것

1차원 데이터 셋에서는 모델이 단순 하므로, 과대적합을 걱정할 필요 가없음

고차원 데이터셋에서는 선형 모델의 성능이 매우 높아져 과대적합될 가능성이 있음



### 2.릿지 회귀

릿지 회귀에서의 가중치(w) 선택은 데이터를 잘 예측하기 위할 뿐더라, 추가 제약조건을 만족시키기 위한 목적도 있음.

평균제곱오차뿐 아니라 w또한 최소화하는 것이 릿지 회귀의 목표



가중치의 절대값을 가능한 한 작게 만드는 것 (다시 말해서 w의 모든 원소가 0에 가깝게..)

모든 특성이 출력에 주는 영향을 최소한으로 만드는 것을 뜻함

=> 이런것을 규제(regularization)

규제란 과대적합이 되지 않도록 모델을 강제로 제한하는 것을 뜻함. 릿지에서 사용하는 규제 방식은 L2 규제



alpha 값이 크다는 것은, 모델이 단순해지는 것을 의미 즉 각 특성이 출력에 영향을 잘 주지 않음을 의미

=> 일반화가 잘됨



### 3.라쏘 회귀

라쏘또한 계수를 0에 가깝게 만드려고 노력함. 하지만 방식이 조금 다르며 L1규제라고 함

L1 규제의 결과로 라쏘를 사용할 때 어떤 계수는 정말 0이됨 (모델에서 완전히 제외되는 특성이 생김)

일부 계수를 0으로 만들면 모델을 이해하기 쉬워지고 이 모델의 가장 중요한 특성이 무엇인지 보여줌



alpha값을 조절하여, 과소적합을 줄인다.(모델을 복잡하게 만듬)



**Overfitting(과대적합)의 문제점을 해결하는 방법**

1. 더 많은 데이터를 사용
2. Cross Validation
3. Regualrization
